---
title: "LoanLogisticRegModel"
author: "Laura Ahumada, Erin McClure-Price, Duy Nguyen"
date: ""
output: "pdf_document"
editor_options: 
  chunk_output_type: inline
---

```{r, setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, comment=NA) 
```
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ROCR)
library(glmnet) 
#Load Libraries
library(Hmisc)
library(gridExtra)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggthemes)
library(class)
library(caret)
theme_set(theme_classic()) #Set the theme for plots
theme_update(plot.title = element_text(hjust = 0.5))
options(scipen=999)

```


# EDA
#### **Looking into the data**

```{r, include=FALSE}
PersonalLoan=read.csv("~/Documents/Statistics 2/StatsProject2/Statistcs2-project-2/Bank_Personal_Loan_Modelling.csv")

# 14 attributes
dim(PersonalLoan)[2]
print("There are 14 Columns")
# 5000 data size
nrow(PersonalLoan)
print("There 5000 entries")
# no duplicates
any(duplicated(PersonalLoan))
print("There are no duplicates")

#No missing values
print("No missing values")
(sapply(PersonalLoan, function(x) all_miss=sum(is.na(x))))

attach(PersonalLoan)
head(PersonalLoan)
```

#### **Statistics**

+ ID can be dropped since it is not a useful predictor and just a unique identifier and will affect the results of the model
+ Age mean is 45 years old, 23 has is the lowest and 67 is the highest. There seems to be a normal distribution.
+ ***There are negative values in Experience which is odd because there can't be negative  years of experience. This will be looked at next***.
+ The mean income is 73 while the median is 64 showing skewness which seems normal representation of society. The minimum income is 8,000 and the max is 224,000. These are common Salaries
+ ZIP.Code should not be numberic they should be changed to categorical. There are 467 distinct ZIP codes
+ There seems to be an equal distribution for families size 1,2,3,4. Each are compose of about 25%
+ The CCAVG, Average Spending per 1000 goes from 0 to 10,000, however the median is 1,5000. Here we can see that there are outliers. 
+ For Education, 41% have have up to highschool, 28% up to under grad studies and interestingly 30% have up to grad school. That seems like a reasonable distribution.
+ For mortgage we can see how skewed it is, the mean is 56.5 and the median is 0. Showing the extreme outliers. **This needs to be looked at**
+ For the target variable, personal loan, we can see quite a difference, 90% without a loan and only 10% with loan. This makes sense because not many people take loan from banks
+ Securities account also has a big difference where 90% does not have a security account while 10% does
+ For CD.Account (Ceritificate deposit) once again we see the 94% does not have it while 6% has it.
+ As for online (online banking capability), 40% doe not have it while 60% has it. That distribution is a little more balanced and makes sense.
+ As for Credit card 70% does not have it while 30% has it. 


```{r, warning = F, echo=FALSE}
print(describe(PersonalLoan))
summary(PersonalLoan)
```


##### Looking into questions obtained in the statistical analysis

+ ***Looking at the entries with negative experience***
+ The 52 people with negative total experience are between 23 to 29 years old and salary median of 65,000. Sound like these could just young people that just started working. We can change their negative values to 1 since they have an income, thus are are working.

```{r, echo=FALSE}
library(dplyr)
#Verifying who are the people with negative experience 
summary(PersonalLoan %>% select (Age,Income,Personal.Loan) %>% filter(Experience<0) )

#Updating values
PersonalLoan$Experience[PersonalLoan$Experience<0]=1
```



```{r, include=FALSE}
# Creating graphs because we will change the variables to categories and this won't work
PersonalLoan = PersonalLoan %>% mutate(Experience2 = Experience/Age)

a=ggplot(PersonalLoan,aes(x=Income,y=Personal.Loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=2)+
  ylim(-.2,1.2)

b=ggplot(PersonalLoan,aes(x=CCAvg,y=Personal.Loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=2)+
  ylim(-.2,1.2)


c=ggplot(PersonalLoan,aes(x=(Mortgage),y=Personal.Loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=2)+
  ylim(-.2,1.2)

d=ggplot(PersonalLoan,aes(x=Experience2,y=Personal.Loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=2)+
  ylim(-.2,1.2)

e=ggplot(PersonalLoan,aes(x=Income,y=Personal.Loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=2)+
  ylim(-.2,1.2)
f=ggplot(PersonalLoan,aes(x=Education,y=Personal.Loan))+geom_point()+
  geom_smooth(method="loess",size=1,span=2)+
  ylim(-.2,1.2)

```

+ Setting categories as factors
+ Creating a new variable called Exprience 2 due to Age and experience having a correlation of 97%. That way we can get rid of Age and Experience
```{r,echo=FALSE}
# Identification Columns (ID and ZIP.Code)
# Droppingzip and ID as there are too many unique values and will affect the model
PersonalLoan = PersonalLoan[-c(1,5)]

# making sure yes is our targer variable
PersonalLoan$Personal.Loan<-factor(ifelse(PersonalLoan$Personal.Loan==1,"Yes","No"),levels=c("No","Yes"))  #last level is the success

# To categorical
factor_vars = c("Family", "Education", 
                "Securities.Account", "CD.Account", "Online", "CreditCard")
PersonalLoan[factor_vars] = lapply(PersonalLoan[factor_vars], as.factor)

# ALREADY CREATED TO DO PLOTS
#PersonalLoan = PersonalLoan %>% mutate(Experience2 = Experience/Age)

# getting rid of Age and Experience
PersonalLoan = PersonalLoan[-c(1,2)] 
```


+ ***Checking mortgage distribution***
+ Mortgage may need to be logged as it is very skewed.


```{r, echo=FALSE}

a1=PersonalLoan %>% ggplot(aes(x=(Mortgage), color="blue")) + geom_boxplot()+ theme_classic()+ theme(legend.position="none") + ggtitle("Distribution of Mortgage")

b1=PersonalLoan %>% ggplot(aes(x=log(Mortgage), color="blue")) + geom_boxplot()+ theme_classic() +theme(legend.position="none")+ ggtitle("Distribution of Mortgage logged")
grid.arrange(a1,b1, ncol=2)
```

+ Creating another data set with Mortgage logged since logging did improve the distribution. The zero's were replaced to with 1 in order to not get infitiy when logging. Also, removing age since it has a 99% correlation with Experience, therefore only one is needed
```{r}
#Creating a new data set with the modified attributes
PersonalLoan2=mutate(PersonalLoan)
#Updating values
PersonalLoan2$Mortgage[PersonalLoan2$Mortgage==0]=1
PersonalLoan2$MortgageLogged=log(PersonalLoan2$Mortgage)
#PersonalLoan2=PersonalLoan2 %>% select(-Mortgage)

```



#### **Relationships and correlations**

+ Experience and Age has a Correlation of 99%.Too high. However, they seem to have no relationship with Loan as both; yes loan and no loan, have the same correlation with experience and age of 0.99 
+ CCAvg and Income has a 64% correlations and it does seems to have a relationship with Loan since there is 0.62 with no loan and .02 with yes loan. 
+ The rest of the explanatory variables do not seem to have relationship between each other


```{r echo=FALSE}
library(GGally)
ggpairs(PersonalLoan,columns = c(1,2,3,5,6,7,8),mapping=aes(colour=Personal.Loan))

library(corrplot)
numData=PersonalLoan %>% select(colnames(PersonalLoan)[!grepl('factor|logical|character',sapply(PersonalLoan,class))])
corrplot(cor(numData), type = "upper", order = "hclust", 
         tl.col = "black", sig.level = 0.05,insig = "blank")
```

#### **Checking realtionship between Loan (response variable) and the rest of the predictors**

##### Very high Relationship present
+ ***Income***, the more income the more changes to get a loan 
+ ***Mortgage***, people high mortgages seem to ask for loans more so than those with lower mortgages


##### Relationship present
+ ***CD.Account*** (certificate of deposit) seem to have a relationship with Personal Loan. Those with personal loan tend to have CD.Account more so than those with no CD.Account
+ ***Personal education***, people with up to highschool tend do not get loans as much as does with an education of higher than highschool
+     !!**there seems to be relationship with loan and education but that of education 2 and 3 seem to be very close so maybe making 2 and 3 one single category would be more helpful**!!

##### Very small relationship
+ ***Security account*** and Personal Loan  seem to have slight relationship

##### No relationship with response
+ ***Online and Credit card*** don't seem to have a relationship

## Looking closer at each realationship to see if anything was missed

```{r, warning=FALSE, message=FALSE, echo=FALSE}
#+ ***Checking relationships  closer with loess***
grid.arrange(a,b,c,d, ncol=2)
grid.arrange(e,f, ncol=2)

```

+ More detailed graph on each variable with response
```{r, echo=FALSE, warning=FALSE}
library(GGally)
library(car) 
#ggpairs(numData, aes(alpha = 0.4))
#pairs(lifeExp[,-c(1,3)],col=Status)


# This determines green is yes.
par(mfrow=c(2,3))
plot(Personal.Loan~.,data=PersonalLoan, col= c("pink","lightblue"))

```



```{r, include=FALSE}
# Looking deeper at relationship between Loan and Securities Account
# There actually is a relationship
#Loan vs securites
a=ggplot(PersonalLoan, aes(x=Securities.Account, fill=Personal.Loan)) + geom_bar(position="dodge")+theme(legend.position="none")+ ggtitle("Security account vs Loan")

b=ggplot(PersonalLoan, aes(x=Online, fill=Personal.Loan)) + geom_bar(position="dodge")+theme(legend.position="none")+ ggtitle("Online access vs Loan")

c=ggplot(PersonalLoan, aes(x=CreditCard, fill=Personal.Loan)) + geom_bar(position="dodge")+theme(legend.position="none")+ ggtitle("Credit Card vs Loan")

grid.arrange(a,b,c, ncol=2)

```


# Model: Objective 1


```{r}
# Train Test Split
set.seed(123)
index<-sample(1:dim(PersonalLoan)[1],round(.70 * dim(PersonalLoan)[1]))
train<-PersonalLoan[index,]
test<-PersonalLoan[-index,]


# Split Predict for lasso
dat.test.x = model.matrix(Personal.Loan ~ Income + Family + CCAvg-1 + Education + Securities.Account-1 + CD.Account-1 + Online-1 + CreditCard -1 + Experience2, test)

dat.train.x = model.matrix(Personal.Loan ~ Income + Family + CCAvg-1 + Education + Securities.Account-1 + CD.Account-1 + Online-1 + CreditCard -1 + Experience2, train)
dat.train.y = train$Personal.Loan

```

***Performing model with all variables, some feature selection methods (forward, stepwise, LASSO) and another based on EDA***
+ With the full model with all attributes it showed that the only important were Income, Family,CCavg, Education,Securites.Account, CD.Account, Online, CreditCard

+ Once Stepwise was added to the full model it selected all of those that appeared as significant in the full model:Income, Family, CCAvg, Education, Securities.Account, CD.Account, Online, CreditCard. It also selected Experience2 but that one was not significant as we had seen in the EDA.

+ When the forward model was added to the full model it selected the same thing as stepwise but included CCAVG which was significant and Mortgage which was not significant .

+ As for LASSO  it selected all of the attributes by Stepwise and included CCAvg.


```{r, echo=FALSE, warning=FALSE, message=FALSE}
# FUNCTIONS TO PREDICT RESULTS 

library(MASS)
library(tidyverse)
library(car)

results <- function(model,bool){

  if (bool==FALSE){
    AIC_result=round((AIC(model)),3)
    BIC_result=round((BIC(model)),3)
    fit.pred<-predict(model,newdata=test,type="response")
    class.model<-factor(ifelse(fit.pred>0.3,"Yes","No"),levels=c("No","Yes"))
    accuracy=round((mean(class.model==test$Personal.Loan)),3)
    Sensitivitylis=round((sensitivity(class.model, test$Personal.Loan)),3)
    specificitlis= round((specificity(class.model, test$Personal.Loan)),3)
  }else{
    AIC_result=0
    BIC_result=0
    fit.pred.lasso = predict(model, newx = dat.test.x, type = "response")
    class.model<-factor(ifelse(fit.pred.lasso>0.3,"Yes","No"),levels=c("No","Yes"))
    accuracy=round((mean(class.model==test$Personal.Loan)),3)
    Sensitivitylis=round((sensitivity(class.model, test$Personal.Loan)),3)
    specificitlis= round((specificity(class.model,test$Personal.Loan)),3)
  }
  all_=c(AIC_result,BIC_result,accuracy,Sensitivitylis,specificitlis)
  return(all_)
 
}


ROC_predict <- function(model, bool){
  fit.pred<-predict(model,newdata=test,type="response")
  results.model<-prediction(fit.pred, test$Personal.Loan,label.ordering=c("No","Yes"))
  roc.model= performance(results.model, measure = "tpr", x.measure = "fpr")
  return(roc.model)
 
}

```



```{r, echo=FALSE, warning=FALSE, message=FALSE}

# RUNNING ALL MODELS

all_results=data.frame(Criterion=c("AIC","BIC","Accuracy","Sensitivity", "Specificity"))

#FULL_MODEL
simpleLG<-glm(Personal.Loan~.,family="binomial",data=train)
all_results$Full_Model=results(simpleLG,FALSE)
roc_full=ROC_predict(simpleLG)

###################################################

# MODEL SELECTION

#STEP_WISE
stepWiseAIC<-simpleLG %>% stepAIC(trace=FALSE, direction = "both")
all_results$Step_Wise=results(stepWiseAIC,FALSE)
roc_stepWiseAIC=ROC_predict(stepWiseAIC)

# FORWARD
ForwardModel <- step(simpleLG, direction = "forward", trace = FALSE)
all_results$Forward_Model=results(ForwardModel,FALSE)
roc_ForwardModel=ROC_predict(ForwardModel)

#####################################################

# LASSO
cvfit = cv.glmnet(dat.train.x, dat.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
#coef(cvfit, s = "lambda.min")
# Optimal penalty
cvfit$lambda.min

# Having found the optinal penalty we creat the mode
# For final model predictions go ahead and refit lasso using entire data set
LASSOmodel<-glmnet(dat.train.x, dat.train.y, family = "binomial",lambda=cvfit$lambda.min)
#CF <- as.matrix(coef(LASSOmodel, LASSOmodel$lambda.1se))
#CF[CF!=0,]
#coef(LASSOmodel)

#Predict
all_results$LASSO_model=results(LASSOmodel,TRUE)

#predict
fit.pred.lasso <- predict(LASSOmodel, newx = dat.test.x, type = "response")
results.lasso<-prediction(fit.pred.lasso, test$Personal.Loan,label.ordering=c("No","Yes"))
roc.lasso = performance(results.lasso, measure = "tpr", x.measure = "fpr")
####################################################

#MODEL BASED ON INTUTION
intuition = glm(formula = Personal.Loan ~ CreditCard + Family + CD.Account + Education + Income,
           family = "binomial", data = train)
all_results$Intuition=results(intuition,FALSE)
roc_Intuition=ROC_predict(intuition)


eda=glm(Personal.Loan~Income+ CD.Account+Mortgage+ Education + Family+ CD.Account, family = "binomial", data = train)
all_results$EDA=results(eda,FALSE)
roc_EDA=ROC_predict(eda)
```

+ Using different threholds 
```{r, echo=FALSE, warning=FALSE, message=FALSE}
threshold <- function(model,bool){
  
  if (bool==FALSE){
    fit.pred<-predict(model,newdata=test,type="response")
  }else{
     #for lasso
    fit.pred = predict(model, newx = dat.test.x, type = "response")
  }
  
  ModelList=c()
  limits=c(0.3,0.5,0.7)
  for (i in  limits){
      lim=i
      class.model<-factor(ifelse(fit.pred>i,"Yes","No"),levels=c("No","Yes"))
      #confusionMatrix(class.model, test$Personal.Loan)
      accuracy=mean(class.model==test$Personal.Loan)
      Sensitivity=(sensitivity(class.model, test$Personal.Loan))
      specificit= (specificity(class.model,test$Personal.Loan))
      ModelList=append(ModelList,list(c(lim,accuracy,Sensitivity,specificit)))
  }
  
  return(ModelList)
 
}
```

***Changing the threshold***
```{r, echo=FALSE}
# Comparing stepwise with different threashold
saving=threshold(simpleLG,FALSE)
names(saving) <- c("0.3", "0.5", "0.7")
print("All_Attributes")
print("Threashhold | Accuracy| Sensitivity| Specificy")
print(saving)
print("--------------------------------")

# Comparing simpleLG with different threashold
saving=threshold(stepWiseAIC,FALSE)
names(saving) <- c("0.3", "0.5", "0.7")
print("StepWiseAIC")
print("Threashhold | Accuracy| Sensitivity| Specificy")
print(saving)
print("--------------------------------")

# Comparing ,ForwardModel with different threashold
saving=threshold(ForwardModel,FALSE)
names(saving) <- c("0.3", "0.5", "0.7")
print("Threashhold | Accuracy| Sensitivity| Specificy")
print("ForwardModel")
print(saving)
print("--------------------------------")

# Comparing LASSOmodel with different threashold
saving=threshold(LASSOmodel,TRUE)
names(saving) <- c("0.3", "0.5", "0.7")
print("Threashhold | Accuracy| Sensitivity| Specificy")
print("LASSO")
print(saving)
print("--------------------------------")

# Comparingintuition,eda with different threashold
saving=threshold(intuition,FALSE)
names(saving) <- c("0.3", "0.5", "0.7")
print("Intuition")
print("Threashhold | Accuracy| Sensitivity| Specificy")
print(saving)
print("--------------------------------")

# Comparing eda with different threashold
saving=threshold(eda,FALSE)
names(saving) <- c("0.3", "0.5", "0.7")
print("EDA")
print("Threashhold | Accuracy| Sensitivity| Specificy")
print(saving)
```
***Choosing 0.3 threshold based of the threashold results.***
***Criterion Comparison of all models***
```{r}
all_results
```


***The ROC of models***

```{r, echo=FALSE}
plot(roc.lasso)
plot(roc_stepWiseAIC, col="red", add=TRUE)
plot(roc_ForwardModel,col="orange", add = TRUE)
plot(roc_Intuition,col="blue",add=TRUE)
plot(roc_full,col="yellow",add=TRUE)
plot(roc_full,col="green",add=TRUE)
legend("bottomright",legend=c("Lasso", "Stepwise","Forward","Intuition","EDA", "All Variables"),col=c("black","red","orange","blue","yellow","green"),lty=1,lwd=1)
abline(a=0, b= 1)
```



***Verify Proportions in test and train manually***
+ Distribution in train and test do represent that of the whole data
```{r, echo=FALSE}
# Holding the upcoming predictions accountable
# all propotions match
print("All data")
prop.table(table(PersonalLoan$Personal.Loan))
print("Train")
prop.table(table(train$Personal.Loan))
print("Test")
prop.table(table(test$Personal.Loan))

# This means that, 
# it is preffered that our predictions are 90% no loan and 10% yes loan.
```

+ Assumptions via PLOTS of selected model, Stepwise and checking VIF
+ Plots look normal and there seems to be multicolinarity among variables based on VIF
```{r}
par(mfrow = c(1, 2))
#Cook's Distance Plot
plot(stepWiseAIC, 4)
#Standardized Residuals vs Leverage
plot(stepWiseAIC, 5)
par(mfrow = c(1, 1))
# vifs
vif(stepWiseAIC)
```





# Conclusion from Part 1
+ The best model was step setting the threshold to 0.3
it gave an sensitivity of 94 and specificity of 72
+ Due to the imbalance of amount of people with loan and without loan we do see we do see that the model favors no loan due to it but 72 compared to the 55 specificity was a great increase. This model is about trying to predict those who will say yes to Loan therefore Specificity is important. 
+ The attributes found useful were : Income, Family,CCavg, Education,Securites.Account, CD.Account, Online, CreditCard
+ The threshold was set to 0.3 and it lead to a Sensitivity of 0.96 and specificy of 0.71
+ These were variables seen in the EDA as related to the loan.
+ Coefficients results:
For every unit increase in income the odd of getting a loan are  e^1.06 times higher
For every unit increase in Family the odd of getting a loan are  e^0.698209 times higher
For every unit increase in CCAvg the odd of getting a loan are  e^0.120635 times higher
For every unit increase in  Education  the odd of getting a loan are  e^1.713690 times higher
For every unit increase in Securities.Account 1  the odd of getting a loan are  e^-0.937183 times less likely
For every unit increase in CD.Account1  the odd of getting a loan are  e^3.840892 times higher
For every unit increase in Online1 the odd of getting a loan are  e^-0.673230 times less likely
For every unit increase in CreditCard1 the odd of getting a loan are  e^ -1.122701 times higher


##############################################################################################3


# MODEL: Part 2

+ Checking for interactions
+ Across all of the plots only one age with mortgage could be the ones with an interaction

```{r, eccho=FALSE}

library(sjPlot)    #For effect plotting
library(sjmisc)    #For effect plotting
library(ResourceSelection)  #Hosmer Lemeshow test
#names(PersonalLoan)

PersonalL=read.csv("~/Documents/Statistics 2/StatsProject2/Statistcs2-project-2/Bank_Personal_Loan_Modelling.csv")

a=ggplot(PersonalL,aes(x=Age,y=Personal.Loan,colour=Education))+geom_point()+
  geom_smooth(method="loess",size=1,span=1.5)+
  ylim(-.2,1.2)+
  facet_wrap(~Personal.Loan)

b=ggplot(PersonalL,aes(x=Age,y=Personal.Loan,colour=Family))+geom_point()+
  geom_smooth(method="loess",size=1,span=1.5)+
  ylim(-.2,1.2)+
  facet_wrap(~Family)

c=ggplot(PersonalL,aes(x=Age,y=Personal.Loan,colour=CCAvg))+geom_point()+
  geom_smooth(method="loess",size=1,span=1.5)+
  ylim(-.2,1.2)+
  facet_wrap(~CCAvg)

d=ggplot(PersonalL,aes(x=Age,y=Personal.Loan,colour=Online))+geom_point()+
  geom_smooth(method="loess",size=1,span=1.5)+
  ylim(-.2,1.2)+
  facet_wrap(~Online)

e=ggplot(PersonalL,aes(x=Mortgage,y=Personal.Loan,colour=Family))+geom_point()+
  geom_smooth(method="loess",size=1,span=1.5)+
  ylim(-.2,1.2)


f=ggplot(PersonalL,aes(x=Age,y=Personal.Loan,colour=CreditCard))+geom_point()+
  geom_smooth(method="loess",size=1,span=1.5)+
  ylim(-.2,1.2)+
  facet_wrap(~CreditCard)

g=ggplot(PersonalL,aes(x=Age,y=Personal.Loan,colour=Securities.Account))+geom_point()+
  geom_smooth(method="loess",size=1,span=1.5)+
  ylim(-.2,1.2)+
  facet_wrap(~Securities.Account)

grid.arrange(a,d, ncol=2)
grid.arrange(b,e, ncol=2)
grid.arrange(f,g, ncol=2)

```

+ **Running the models**
+ In part 1 we saw that, Income, Family, CCAvg, CD.Account, Education and Credit Card were significant, therefore we leverage those varibles and kept mortgage (just like in part 1) based on the EDA analysis, for Part 2

***Interaction***

+ All variables:Income, Family, CCAvg, CD.Account, Education, and Credit Card and the in the interaction of family and mortgage were significant.
+ When passing the model model through stepwise and forward it kept the all variables including the interaction and thus kept the same significant variables showing signs that the interaction is indeed useful.
+ When Anova was applied, it did show the interaction as significant 
+ The Hoslem test however showed that the model was a poor fit, yes again this is not reliable due to the size of the data (This I asked in class and Dr.Turner said we couldn't relly on this metric with big data)

***Logged***
+ We logged morgage
+ All variables were significant: Income, Family, CCAvg, CD.Account, Education, and Credit Card including the log mortgage and log income
+ Once the models were mixed, having logged income and the interaction of logged mortgage with family lead to the interaction no longer being significant however all other variable remained significant.
+ LDA mode and QDA model did poorly howwever between the LDA model out performed QDA


```{r, echo=FALSE}
# Train Test Split
index<-sample(1:dim(PersonalLoan2)[1],round(.70 * dim(PersonalLoan2)[1]))
train<-PersonalLoan2[index,]
test<-PersonalLoan2[-index,]


# RUNNING ALL MODELS

all_results2=data.frame(Criterion=c("AIC","BIC","Accuracy","Sensitivity", "Specificity"))

#INTERACTION
ComplexInteractionl<-glm(Personal.Loan~Securities.Account+CD.Account+CreditCard+Education+Income+CCAvg+Family*Mortgage,data=train,family="binomial")   #complex
summary(ComplexInteractionl)
all_results2$Interaction=results(ComplexInteractionl,FALSE)
roc_interaction=ROC_predict(ComplexInteractionl)

#MODEL SELCT TO FILTER

#Performing stepwise to see if it keeps the interaction
stepComplexinteraction<-ComplexInteractionl %>% stepAIC(trace=FALSE, direction="both")
  #summary(stepComplexinteraction)
  #Anova(stepComplexinteraction,type=3)  
  #hoslem.test(stepComplexinteraction$y,fitted(stepComplexinteraction))
#all_results2$stepInteraction=results(stepComplexinteraction,FALSE)
roc_interactionStepwise=ROC_predict(stepComplexinteraction)

interaction_Forward <- step(ComplexInteractionl, direction = "forward", trace = FALSE)
  #summary(interaction_Forward)
  #Anova(interaction_Forward,type=3) 
  #hoslem.test(interaction_Forward$y,fitted(interaction_Forward))
#all_results2$InteractionForward=results(interaction_Forward,FALSE)
roc_interactionForward=ROC_predict(interaction_Forward)

#######################################

# LOGGING INOCME AND USING  MORTGAGELOGGED
Personal.Loan~Securities.Account+CD.Account+CreditCard+Education+Income+CCAvg+Family*Mortgage

ComplexLog<-glm(Personal.Loan~Education+log(Income)+CCAvg+ Family+ MortgageLogged+Securities.Account+CD.Account+CreditCard,data=train,family="binomial")   #complex
#stepComplexlog<-ComplexLog %>% stepAIC(trace=FALSE) # Kept all variables
all_results2$LogIncomeMortgage=results(ComplexLog,FALSE)
roc_logIncomeMortgage=ROC_predict(ComplexLog)

# LOGGING AND INTERACTION
ComplexLogInteraction<-glm(Personal.Loan~Education+log(Income)+CCAvg+ Family*MortgageLogged+Securities.Account+CD.Account+CreditCard,data=train,family="binomial")   #complex
stepComplexLoginteraction<-ComplexLogInteraction %>% stepAIC(trace=FALSE)
#Anova(ComplexLogInteraction,type=3) 
#hoslem.test(stepComplexLoginteraction$y,fitted(stepComplexLoginteraction))
all_results2$LogAndInteraction=results(ComplexLogInteraction,FALSE)
roc_logAndInteraction=ROC_predict(ComplexLogInteraction)


#LDA
lda_<- lda( Personal.Loan ~ Income + CCAvg  + Family + Education +
    Securities.Account + CD.Account + CreditCard + MortgageLogged, 
    data = train)
#myldawPrior<- lda(Personal.Loan ~ X1 + X2, data = PersonalLoan2, prior = c(.20,.80))
pred<-predict(lda_,newdata=test)$class
#plot(pred)
accuracy=round((mean(pred==test$Personal.Loan)),3)
Sensitivit=round((sensitivity(pred, test$Personal.Loan)),3)
specificit= round((specificity(pred, test$Personal.Loan)),3)
all_results2$LDA=c(0,0,accuracy,Sensitivit,specificit)
fit.p<-predict(lda_,newdata=test)
results.model<-prediction(fit.p$posterior[,2], test$Personal.Loan,label.ordering=c("No","Yes"))
roc.lda_= performance(results.model, measure = "tpr", x.measure = "fpr")


#LDA ALL VARIABLES
lda_2<- lda( Personal.Loan ~ Income + CCAvg  + Family + Education +
    Securities.Account + CD.Account + CreditCard + MortgageLogged + Experience2 + Online, 
    data = train)
#lda_2 <- lda(Personal.Loan ~ ., data = train, CV = TRUE)
pred_lda2<-predict(lda_2,newdata=test)$class
accuracy=round((mean(pred_lda2==test$Personal.Loan)),3)
Sensitivit=round((sensitivity(pred_lda2, test$Personal.Loan)),3)
specificit= round((specificity(pred_lda2, test$Personal.Loan)),3)
#all_results2$lda_2=c(0,0,accuracy,Sensitivit,specificit)
fit.p<-predict(lda_2,newdata=test)
results.model<-prediction(fit.p$posterior[,2], test$Personal.Loan,label.ordering=c("No","Yes"))
roc.lda_2= performance(results.model, measure = "tpr", x.measure = "fpr")


#QLA
#qda_mod2 <- qda(Personal.Loan ~ ., data = train, CV = TRUE)
#LDA
qda_1<- qda( Personal.Loan ~ Income + CCAvg  + Education +
    Securities.Account + CD.Account + CreditCard + MortgageLogged, 
    data = train)
#myldawPrior<- lda(Personal.Loan ~ X1 + X2, data = PersonalLoan2, prior = c(.20,.80))
pred<-predict(qda_1,newdata=test)$class  
#plot(pred)
accuracy=round((mean(pred==test$Personal.Loan)),3)
Sensitivit=round((sensitivity(pred, test$Personal.Loan)),3)
specificit= round((specificity(pred, test$Personal.Loan)),3)
#all_results2$qda_1=c(0,0,accuracy,Sensitivit,specificit)
fit.p<-predict(qda_1,newdata=test)
results.model<-prediction(fit.p$posterior[,2], test$Personal.Loan,label.ordering=c("No","Yes"))
roc.qda_1= performance(results.model, measure = "tpr", x.measure = "fpr")

#LDA
qda_2<- qda( Personal.Loan ~ Income + CCAvg  + Family + Education +
    Securities.Account + CD.Account + CreditCard + MortgageLogged + Experience2 + Online, 
    data = train)
#myldawPrior<- lda(Personal.Loan ~ X1 + X2, data = PersonalLoan2, prior = c(.20,.80))
pred<-predict(qda_2,newdata=test)$class  
#plot(pred)
accuracy=round((mean(pred==test$Personal.Loan)),3)
Sensitivit=round((sensitivity(pred, test$Personal.Loan)),3)
specificit= round((specificity(pred, test$Personal.Loan)),3)
all_results2$QDA2=c(0,0,accuracy,Sensitivit,specificit)
fit.p<-predict(qda_2,newdata=test)
results.model<-prediction(fit.p$posterior[,2], test$Personal.Loan,label.ordering=c("No","Yes"))
roc.qda_2= performance(results.model, measure = "tpr", x.measure = "fpr")


```

+ **The ROC of models**

```{r, echo=FALSE}
plot(roc_interaction)
plot(roc_interactionStepwise, col="red", add=TRUE)
plot(roc_interactionForward,col="orange", add = TRUE)
plot(roc_logIncomeMortgage,col="blue",add=TRUE)
plot(roc_logAndInteraction,col="yellow",add=TRUE)
legend("bottomright",legend=c("Interaction", "InteractionStepwise","InteractionForward","LogIncomeAndMortgage","log and Interaction"),col=c("black","red","orange","blue","yellow"),lty=1,lwd=1)
abline(a=0, b= 1)
```


+ ***Log and interaction and log of mortage and income seemed to be the best ones ***
```{r, echo=FALSE}

plot(roc_interaction)
plot(roc_logIncomeMortgage,col="red",add=TRUE)
plot(roc_logAndInteraction,col="orange",add=TRUE)
plot(roc.lda_,col="blue",add=TRUE)
plot(roc.qda_2,col="yellow",add=TRUE)
plot(roc_stepWiseAIC, col="pink", add=TRUE)
legend("bottomright",legend=c("Interaction", "LogIncomeAndMortgage","log and Interaction","LDA", "QDA","StepWise"),col=c("black","red","orange","blue","yellow","pink"),lty=1,lwd=1)
abline(a=0, b= 1)
```

```{r, include=FALSE}

plot(roc.lda_)
plot(roc.lda_2, col="red", add=TRUE)
plot(roc.qda_1,col="orange", add = TRUE)
plot(roc.qda_2,col="blue",add=TRUE)
legend("bottomright",legend=c("LDA Important Var", "LDA_allVariable","QDA important variables","QLD All variables"),col=c("black","red","orange","blue"),lty=1,lwd=1)
abline(a=0, b= 1)
```

```{r}
all_results2

```

***Duy's model****
```{r}
all_results3=data.frame(Criterion=c("AIC","BIC","Accuracy","Sensitivity", "Specificity"))

model.poly.income2 = glm(formula = Personal.Loan ~ poly(Income, 2) + Family + CCAvg + Education + Securities.Account + CD.Account + MortgageLogged + CreditCard, family = "binomial", data = train)
all_results3$polyIncome2=results(model.poly.income2,FALSE)
roc_PolyIncome2=ROC_predict(model.poly.income2)


model.poly.income3 = glm(formula = Personal.Loan ~ poly(Income, 3) + Family + CCAvg + Education + Securities.Account + CD.Account + MortgageLogged + CreditCard, family = "binomial", data = train)
all_results3$polyIncome3=results(model.poly.income3,FALSE)
roc_PolyIncome3=ROC_predict(model.poly.income3)

model.poly.CCAvg2 = glm(formula = Personal.Loan ~ Income + Family + poly(CCAvg, 2) + Education + Securities.Account + CD.Account + Online + MortgageLogged, family = "binomial", data = train)
all_results3$polyCCAvg2=results(model.poly.CCAvg2,FALSE)
roc_CCAvg2=ROC_predict(model.poly.CCAvg2)

model.poly.CCAvg3 = glm(formula = Personal.Loan ~ Income + Family + poly(CCAvg, 2) + Education + Securities.Account + CD.Account + Online + MortgageLogged, family = "binomial", data = train)
all_results3$polyCCAvg3=results(model.poly.CCAvg3,FALSE)
roc_CCAvg3=ROC_predict(model.poly.CCAvg3)

model.poly.both2 = glm(formula = Personal.Loan ~ poly(Income, 2) + Family + poly(CCAvg, 2) + Education + Securities.Account + CD.Account + MortgageLogged + CreditCard, family = "binomial", data = train)
all_results3$PolyBoth2=results(model.poly.both2,FALSE)
roc_PolyBoth2=ROC_predict(model.poly.both2)

model.poly.both3 = glm(formula = Personal.Loan ~ poly(Income, 3) + Family + poly(CCAvg, 3) + Education + Securities.Account + CD.Account + MortgageLogged + CreditCard, family = "binomial", data = train)
all_results3$PolyBoth3=results(model.poly.both3,FALSE)
roc_PolyBoth3=ROC_predict(model.poly.both3)

all_results3

```



plot(roc.poly.income2, col = "green", add = TRUE, xlim = c(0, 0.3), ylim = c(0.7, 1.0))
plot(roc.poly.income3, col = "blue", add = TRUE, xlim = c(0, 0.3), ylim = c(0.7, 1.0))
plot(roc.poly.CCAvg2, col = "blueviolet", add = TRUE, xlim = c(0, 0.3), ylim = c(0.7, 1.0))
plot(roc.poly.CCAvg3, col = "aquamarine", add = TRUE, xlim = c(0, 0.3), ylim = c(0.7, 1.0))
plot(roc.poly.both2, col = "chartreuse", add = TRUE, xlim = c(0, 0.3), ylim = c(0.7, 1.0))
plot(roc.poly.both3, col = "coral", add = TRUE, xlim = c(0, 0.3), ylim = c(0.7, 1.0))
legend("bottomright", legend = c( "polyincome2", "polyincome3", "polyCCAVg2", "polyCCAvg3", "polyboth2", "polyboth3"), 
       col = c( "green", "blue", "blueviolet", "aquamarine", "chartreuse", "coral"), 
       lty=1, lwd=1)





```{r}


# Load the library
#library(randomForest)

#randomFrst <- randomForest(Personal.Loan ~ ., data = PersonalLoan,  importance = TRUE, proximity = TRUE)
```

```{r}
##the normalization function is created
# nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
 
 ##Run nomalization on first 4 coulumns of dataset because they are the predictors
 #iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], nor))
 
  ##run knn function
 #pr <- knn(iris_train,iris_test,cl=iris_target_category,k=13)
 
```

